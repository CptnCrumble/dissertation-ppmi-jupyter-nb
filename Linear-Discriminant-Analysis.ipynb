{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "749b57ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "def build_confusion_matrix(dataset, model):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=0.25)\n",
    "        predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "        tn, fn, fp, tp = confusion_matrix(predictions, y_test.values).ravel()\n",
    "        return {'true_negative': tn, 'false_negative': fn, 'false_positive': fp, 'true_positive': tp}\n",
    "\n",
    "def sensitivity(cm):\n",
    "    return cm['true_positive'] / (cm['true_positive'] + cm['false_negative'])\n",
    "\n",
    "def specificity(cm):\n",
    "    return cm['true_negative'] / ( cm['true_negative'] + cm['false_positive'] )\n",
    "\n",
    "def false_positive_rate(cm):\n",
    "    return (1.0 - (specificity(cm)) )\n",
    "\n",
    "def generate_scores(dataset, model):\n",
    "    specificity_scores = []\n",
    "    sensitivity_scores = []\n",
    "\n",
    "    for n in range(1000):\n",
    "        cm = build_confusion_matrix(dataset, model)\n",
    "        specificity_scores.append(specificity(cm))\n",
    "        sensitivity_scores.append(sensitivity(cm))\n",
    "    \n",
    "    spec_avg = pd.Series(specificity_scores).mean()\n",
    "    sens_avg = pd.Series(sensitivity_scores).mean()\n",
    "    \n",
    "    return {'specificity': spec_avg, 'sensitivity': sens_avg }\n",
    "\n",
    "def summarise_test(test):\n",
    "    x ='For {}, in model {}\\nAverage NO SIGFALL accuracy is {}\\nAverage SIGFALL accuracy is {}\\n---\\n'\n",
    "    print(x.format(test['data_name'],test['model'],test['specificity'],test['sensitivity']))\n",
    "\n",
    "def balance_data_set(dataset, classifier):\n",
    "    sigfall_indexes = dataset.index[dataset[classifier] == 0].to_list()\n",
    "    n_falls = len(dataset) - len(sigfall_indexes)\n",
    "    drop = len(sigfall_indexes) - n_falls\n",
    "    random.shuffle(sigfall_indexes)\n",
    "    drop_indexes = sigfall_indexes[0:drop]\n",
    "    return dataset.drop(drop_indexes)\n",
    "\n",
    "# Balance_data_generate_scores() is a seperate function to invoke a new randomly balanced dataset in each iteration.\n",
    "# If a dataset was pre-balanced then run through generate_scores() it would overfit to that specific dataset \n",
    "\n",
    "def balance_data_generate_scores(dataset, classifier, model):\n",
    "    specificity_scores = []\n",
    "    sensitivity_scores = []\n",
    "\n",
    "    for n in range(1000):\n",
    "        b_dataset = balance_data_set(dataset, classifier)\n",
    "        cm = build_confusion_matrix(b_dataset, model)\n",
    "        specificity_scores.append(specificity(cm))\n",
    "        sensitivity_scores.append(sensitivity(cm))\n",
    "    \n",
    "    spec_avg = pd.Series(specificity_scores).mean()\n",
    "    sens_avg = pd.Series(sensitivity_scores).mean()\n",
    "    \n",
    "    return {'specificity': spec_avg, 'sensitivity': sens_avg }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "596aba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data sets\n",
    "inc_updrs_sigfall = pd.read_csv('./working_data/normalised_increase_updrs_sigfall.csv')\n",
    "inc_updrs_sigfall_raw = pd.read_csv('./working_data/normalised_increase_updrs_sigfall_raw.csv')\n",
    "\n",
    "datasets = [{'data':inc_updrs_sigfall, 'name':'inc_updrs_sigfall'},\n",
    "            {'data':inc_updrs_sigfall_raw, 'name':'inc_updrs_sigfall_raw'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7b3fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models \n",
    "models = [\n",
    "    LinearDiscriminantAnalysis(solver=\"svd\"),\n",
    "    LinearDiscriminantAnalysis(solver=\"lsqr\"),\n",
    "    LinearDiscriminantAnalysis(solver=\"eigen\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b740dd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For inc_updrs_sigfall, in model LinearDiscriminantAnalysis()\n",
      "Average NO SIGFALL accuracy is 0.9783344747015499\n",
      "Average SIGFALL accuracy is 0.12102639323530176\n",
      "---\n",
      "\n",
      "For inc_updrs_sigfall_b, in model LinearDiscriminantAnalysis()\n",
      "Average NO SIGFALL accuracy is 0.7701675882609793\n",
      "Average SIGFALL accuracy is 0.5414108910159185\n",
      "---\n",
      "\n",
      "For inc_updrs_sigfall_raw, in model LinearDiscriminantAnalysis()\n",
      "Average NO SIGFALL accuracy is 0.9790587162492428\n",
      "Average SIGFALL accuracy is 0.11451312778477331\n",
      "---\n",
      "\n",
      "For inc_updrs_sigfall, in model LinearDiscriminantAnalysis(solver='lsqr')\n",
      "Average NO SIGFALL accuracy is 0.9787376983900437\n",
      "Average SIGFALL accuracy is 0.11924606189749122\n",
      "---\n",
      "\n",
      "For inc_updrs_sigfall_b, in model LinearDiscriminantAnalysis(solver='lsqr')\n",
      "Average NO SIGFALL accuracy is 0.7746724080271306\n",
      "Average SIGFALL accuracy is 0.5406483697831564\n",
      "---\n",
      "\n",
      "For inc_updrs_sigfall_raw, in model LinearDiscriminantAnalysis(solver='lsqr')\n",
      "Average NO SIGFALL accuracy is 0.9782692943459869\n",
      "Average SIGFALL accuracy is 0.11750076266227943\n",
      "---\n",
      "\n",
      "For inc_updrs_sigfall, in model LinearDiscriminantAnalysis(solver='eigen')\n",
      "Average NO SIGFALL accuracy is 0.9782924032805671\n",
      "Average SIGFALL accuracy is 0.11946763181143222\n",
      "---\n",
      "\n",
      "For inc_updrs_sigfall_b, in model LinearDiscriminantAnalysis(solver='eigen')\n",
      "Average NO SIGFALL accuracy is 0.7664355150305058\n",
      "Average SIGFALL accuracy is 0.5450229668767149\n",
      "---\n",
      "\n",
      "For inc_updrs_sigfall_raw, in model LinearDiscriminantAnalysis(solver='eigen')\n",
      "Average NO SIGFALL accuracy is 0.9786813877882424\n",
      "Average SIGFALL accuracy is 0.1158386412132879\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run tests\n",
    "\n",
    "tests = []\n",
    "for m in models:\n",
    "    for d in datasets:\n",
    "        tests.append({\n",
    "            'data_name': d['name'],\n",
    "            'dataset': d['data'],\n",
    "            'model': m\n",
    "        })\n",
    "\n",
    "for t in tests:\n",
    "    s = generate_scores(dataset=t['dataset'], model=t['model'])\n",
    "    t['specificity'] = s['specificity']\n",
    "    t['sensitivity'] = s['sensitivity']\n",
    "    summarise_test(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b7eb939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'specificity': 0.9785999321848587, 'sensitivity': 0.1148227988475207}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See QDA notebook for implementation of 'Balanced Datasets' - not sure its a valid thing to do ATM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
