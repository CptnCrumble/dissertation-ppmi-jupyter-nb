{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b08d9132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "def build_confusion_matrix(dataset, model):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size=0.25)\n",
    "        predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "        tn, fn, fp, tp = confusion_matrix(predictions, y_test.values).ravel()\n",
    "        return {'true_negative': tn, 'false_negative': fn, 'false_positive': fp, 'true_positive': tp}\n",
    "\n",
    "def sensitivity(cm):\n",
    "    return cm['true_positive'] / (cm['true_positive'] + cm['false_negative'])\n",
    "\n",
    "def specificity(cm):\n",
    "    return cm['true_negative'] / ( cm['true_negative'] + cm['false_positive'] )\n",
    "\n",
    "def false_positive_rate(cm):\n",
    "    return (1.0 - (specificity(cm)) )\n",
    "\n",
    "def generate_scores(dataset, model):\n",
    "    specificity_scores = []\n",
    "    sensitivity_scores = []\n",
    "\n",
    "    for n in range(1000):\n",
    "        cm = build_confusion_matrix(dataset, model)\n",
    "        specificity_scores.append(specificity(cm))\n",
    "        sensitivity_scores.append(sensitivity(cm))\n",
    "    \n",
    "    spec_avg = pd.Series(specificity_scores).mean()\n",
    "    sens_avg = pd.Series(sensitivity_scores).mean()\n",
    "    \n",
    "    return {'specificity': spec_avg, 'sensitivity': sens_avg }\n",
    "\n",
    "def summarise_test(test):\n",
    "    x ='For {}, in model {}\\nAverage NO SIGFALL accuracy is {}\\nAverage SIGFALL accuracy is {}\\n---\\n'\n",
    "    print(x.format(test['data_name'],test['model'],test['specificity'],test['sensitivity']))    \n",
    "    \n",
    "def balance_data_set(dataset, classifier):\n",
    "    sigfall_indexes = dataset.index[dataset[classifier] == 0].to_list()\n",
    "    n_falls = len(dataset) - len(sigfall_indexes)\n",
    "    drop = len(sigfall_indexes) - n_falls\n",
    "    random.shuffle(sigfall_indexes)\n",
    "    drop_indexes = sigfall_indexes[0:drop]\n",
    "    return dataset.drop(drop_indexes)\n",
    "\n",
    "# Balance_data_generate_scores() is a seperate function to invoke a new randomly balanced dataset in each iteration.\n",
    "# If a dataset was pre-balanced then run through generate_scores() it would overfit to that specific dataset \n",
    "\n",
    "def balance_data_generate_scores(dataset, classifier, model):\n",
    "    specificity_scores = []\n",
    "    sensitivity_scores = []\n",
    "\n",
    "    for n in range(1000):\n",
    "        b_dataset = balance_data_set(dataset, classifier)\n",
    "        cm = build_confusion_matrix(b_dataset, model)\n",
    "        specificity_scores.append(specificity(cm))\n",
    "        sensitivity_scores.append(sensitivity(cm))\n",
    "    \n",
    "    spec_avg = pd.Series(specificity_scores).mean()\n",
    "    sens_avg = pd.Series(sensitivity_scores).mean()\n",
    "    \n",
    "    return {'specificity': spec_avg, 'sensitivity': sens_avg }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a25e7945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data sets\n",
    "inc_updrs_sigfall = pd.read_csv('./working_data/normalised_increase_updrs_sigfall.csv')\n",
    "inc_updrs_sigfall_raw = pd.read_csv('./working_data/normalised_increase_updrs_sigfall_raw.csv')\n",
    "\n",
    "datasets = [{'data':inc_updrs_sigfall, 'name':'inc_updrs_sigfall'}, \n",
    "            {'data':inc_updrs_sigfall_raw, 'name':'inc_updrs_sigfall_raw'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57f376cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For inc_updrs_sigfall, in model QuadraticDiscriminantAnalysis(store_covariance=True)\n",
      "Average NO SIGFALL accuracy is 0.849244571426317\n",
      "Average SIGFALL accuracy is 0.39343836216742456\n",
      "---\n",
      "\n",
      "For inc_updrs_sigfall_raw, in model QuadraticDiscriminantAnalysis(store_covariance=True)\n",
      "Average NO SIGFALL accuracy is 0.8563806764473703\n",
      "Average SIGFALL accuracy is 0.3780726791459436\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run tests to score each model & dataset combination\n",
    "\n",
    "models = [QuadraticDiscriminantAnalysis(store_covariance=True)]\n",
    "\n",
    "tests = []\n",
    "for m in models:\n",
    "    for d in datasets:\n",
    "        tests.append({\n",
    "            'data_name': d['name'],\n",
    "            'dataset': d['data'],\n",
    "            'model': m\n",
    "        })\n",
    "\n",
    "for t in tests:\n",
    "    s = generate_scores(dataset=t['dataset'], model=t['model'])\n",
    "    t['specificity'] = s['specificity']\n",
    "    t['sensitivity'] = s['sensitivity']\n",
    "    summarise_test(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8787d8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:873: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For balanced_inc_updrs_sigfall, in model QuadraticDiscriminantAnalysis(store_covariance=True)\n",
      "Average NO SIGFALL accuracy is 0.6933489846644311\n",
      "Average SIGFALL accuracy is 0.6319434743656722\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:873: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:873: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:898: RuntimeWarning: divide by zero encountered in power\n",
      "  X2 = np.dot(Xm, R * (S ** (-0.5)))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:898: RuntimeWarning: invalid value encountered in multiply\n",
      "  X2 = np.dot(Xm, R * (S ** (-0.5)))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/discriminant_analysis.py:901: RuntimeWarning: divide by zero encountered in log\n",
      "  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For balanced_inc_updrs_sigfall_raw, in model QuadraticDiscriminantAnalysis(store_covariance=True)\n",
      "Average NO SIGFALL accuracy is 0.6971075020855515\n",
      "Average SIGFALL accuracy is 0.6258116969941095\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run tests to score each model with the randomly balanced version of the datasets\n",
    "\n",
    "balanced_datasets = [{'data':inc_updrs_sigfall, 'name':'balanced_inc_updrs_sigfall', 'classifier':'SIGFALL'}, \n",
    "                     {'data':inc_updrs_sigfall_raw, 'name':'balanced_inc_updrs_sigfall_raw', 'classifier':'SIGFALL'}]\n",
    "\n",
    "b_tests = []\n",
    "for m in models:\n",
    "    for d in balanced_datasets:\n",
    "        b_tests.append({\n",
    "            'data_name': d['name'],\n",
    "            'dataset': d['data'],\n",
    "            'classifier': d['classifier'],\n",
    "            'model': m\n",
    "        })\n",
    "\n",
    "for t in b_tests:\n",
    "    s = balance_data_generate_scores(dataset=t['dataset'], classifier=d['classifier'], model=t['model'])\n",
    "    t['specificity'] = s['specificity']\n",
    "    t['sensitivity'] = s['sensitivity']\n",
    "    summarise_test(t)       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
